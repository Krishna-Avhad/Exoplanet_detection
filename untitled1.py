# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rl0N19oHE_x7uU4SyoPV9LN6u1Afy_ot
"""




from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Folder path in Google Drive
path = '/content/drive/MyDrive/ExoplanetData/'

# Load full datasets (no row limits)
train_df = pd.read_csv(path + 'exoTrain.csv')
test_df = pd.read_csv(path + 'exoTest.csv')
cumulative_df = pd.read_csv(path + 'cumulative.csv')

print("Train shape:", train_df.shape)
print("Test shape:", test_df.shape)
print("Cumulative shape:", cumulative_df.shape)
train_df.head()

import matplotlib.pyplot as plt
import numpy as np

# label distribution
print(train_df['LABEL'].value_counts(normalize=True))

# plot a few sample light curves
for i in range(3):
    sample = train_df.drop(columns='LABEL').iloc[i].values
    label = train_df['LABEL'].iloc[i]
    plt.figure(figsize=(7,2))
    plt.plot(sample)
    plt.title(f"Sample {i} - LABEL={label}")
    plt.xlabel("Time step")
    plt.ylabel("Flux")
    plt.show()

# Separate features & labels
X_train = train_df.drop(columns='LABEL').values.astype('float32')
y_train = train_df['LABEL'].values.astype('int')

X_test = test_df.drop(columns='LABEL').values.astype('float32')
y_test = test_df['LABEL'].values.astype('int')

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

# For CNN, reshape
X_train_cnn = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_cnn  = X_test_scaled.reshape((X_test_scaled.shape[0],  X_test_scaled.shape[1], 1))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf.fit(X_train_scaled, y_train)

y_pred = rf.predict(X_test_scaled)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks

# reshape for Conv1D: (samples, time_steps, channels)
X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_cnn  = X_test.reshape((X_test.shape[0],  X_test.shape[1],  1))

model = models.Sequential([
  layers.Conv1D(32, kernel_size=5, activation='relu', input_shape=(X_train.shape[1],1)),
  layers.BatchNormalization(),
  layers.MaxPooling1D(2),
  layers.Conv1D(64, kernel_size=5, activation='relu'),
  layers.BatchNormalization(),
  layers.MaxPooling1D(2),
  layers.Conv1D(128, kernel_size=5, activation='relu'),
  layers.MaxPooling1D(2),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dropout(0.5),
  layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# callbacks: early stopping + save best
es = callbacks.EarlyStopping(patience=5, restore_best_weights=True)
ckpt_path = path + 'best_cnn.h5'
mc = callbacks.ModelCheckpoint(ckpt_path, save_best_only=True)

history = model.fit(
    X_train_cnn, y_train,
    validation_split=0.1,
    epochs=30,
    batch_size=64,
    callbacks=[es, mc]
)

# Evaluate
model.load_weights(ckpt_path)
loss, acc = model.evaluate(X_test_cnn, y_test, verbose=0)
print("CNN test accuracy:", acc)

# Per-sample normalization
X_train_norm = (X_train - X_train.mean(axis=1, keepdims=True)) / (X_train.std(axis=1, keepdims=True) + 1e-8)
X_test_norm = (X_test - X_test.mean(axis=1, keepdims=True)) / (X_test.std(axis=1, keepdims=True) + 1e-8)

X_train_cnn = X_train_norm.reshape((X_train_norm.shape[0], X_train_norm.shape[1], 1))
X_test_cnn  = X_test_norm.reshape((X_test_norm.shape[0],  X_test_norm.shape[1],  1))

# Save sklearn model
import joblib
joblib.dump(rf, path + 'rf_model.joblib')

# Keras model already saved (best_cnn.h5). To save final:
model.save(path + 'final_cnn_model.keras')

# Save predictions csv
import pandas as pd
out = pd.DataFrame({'true': y_test, 'rf_prob': rf_probs, 'cnn_prob': cnn_probs})
out.to_csv(path + 'test_predictions.csv', index=False)
print("Saved predictions to Drive.")

pred_probs = model.predict(X_test_cnn).ravel()
pred_labels = (pred_probs > 0.5).astype(int)

for i in np.random.choice(len(X_test_cnn), 3, replace=False):
    plt.figure(figsize=(8,2))
    plt.plot(X_test_cnn[i])
    plt.title(f"True: {y_test[i]} | Predicted: {pred_labels[i]} | Prob: {pred_probs[i]:.2f}")
    plt.show()

out = pd.DataFrame({'true': y_test, 'rf_prob': rf.predict_proba(X_test_scaled)[:,1], 'cnn_prob': pred_probs})
out.to_csv(path + 'test_predictions.csv', index=False)
print("Saved predictions to Drive.")

counts = cumulative_df['koi_disposition'].value_counts()
plt.figure(figsize=(6,4))
counts.plot(kind='bar', color=['green','orange','red'])
plt.title('Kepler Object Disposition Count')
plt.xlabel('Disposition Type')
plt.ylabel('Number of Objects')
plt.show()

# Example scatter plots
plt.figure(figsize=(8,6))
plt.scatter(cumulative_df['koi_period'], cumulative_df['koi_prad'], alpha=0.5, c='skyblue')
plt.xscale('log')
plt.xlabel('Orbital Period (days)')
plt.ylabel('Planet Radius (Earth radii)')
plt.title('Planet Radius vs Orbital Period')
plt.show()

plt.figure(figsize=(8,6))
plt.scatter(cumulative_df['koi_steff'], cumulative_df['koi_teq'], alpha=0.5, c='orange')
plt.xlabel('Star Temperature (K)')
plt.ylabel('Planet Equilibrium Temp (K)')
plt.title('Star vs Planet Temperature')
plt.show()

# Habitable candidates
habitable = cumulative_df[(cumulative_df['koi_teq'] > 200) & (cumulative_df['koi_teq'] < 350) & (cumulative_df['koi_prad'] < 2)]
print("Possible habitable exoplanets:")
print(habitable[['kepler_name','koi_teq','koi_prad','koi_disposition']].head())

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.show()

